import gradio as gr
from google import genai
# ã€V16 ä¿®å¤ã€‘ä¸¥æ ¼æŒ‰ç…§æ–‡æ¡£å’Œæ‚¨çš„æŒ‡ç¤ºï¼Œå¯¼å…¥ types
from google.genai import types as genai_types
from pydantic import BaseModel, Field, ValidationError, TypeAdapter
from typing import List, Optional
import os
import time
import json
from PIL import Image
import numpy as np # ç¡®ä¿å¯¼å…¥ numpy

# --- 1. UI/UX å¢å¼º: è‡ªå®šä¹‰CSSä¸å“ç‰Œå…ƒç´  ---
GEMINI_LOGO_SVG = """<svg width="30px" height="30px" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg" style="display: inline-block; vertical-align: middle; margin-right: 8px;"><path d="M12 21.5C12 21.5 11.5 18.5 12 15.5C12.5 12.5 15 11.5 18 11.5C21 11.5 21.5 14 21.5 15.5C21.5 17 21.5 21.5 21.5 21.5C21.5 21.5 17.5 21.5 12 21.5Z" fill="url(#paint0_linear_10_23)"></path><path d="M2.5 12C2.5 12 5.5 11.5 8.5 12C11.5 12.5 12.5 15 12.5 18C12.5 21 10 21.5 8.5 21.5C7 21.5 2.5 21.5 2.5 21.5C2.5 21.5 2.5 17.5 2.5 12Z" fill="url(#paint1_linear_10_23)"></path><path d="M8.5 2.5C8.5 2.5 11 3 12 6C13 9 11.5 10 8.5 10C5.5 10 2.5 9.5 2.5 8C2.5 6.5 2.5 2.5 2.5 2.5C2.5 2.5 4 2.5 8.5 2.5Z" fill="url(#paint2_linear_10_23)"></path><defs><linearGradient id="paint0_linear_10_23" x1="12" y1="11.5" x2="21.5" y2="21.5" gradientUnits="userSpaceOnUse"><stop stop-color="#89B5F7"></stop><stop offset="1" stop-color="#4285F4"></stop></linearGradient><linearGradient id="paint1_linear_10_23" x1="2.5" y1="12" x2="12.5" y2="21.5" gradientUnits="userSpaceOnUse"><stop stop-color="#FCD26A"></stop><stop offset="1" stop-color="#FABB05"></stop></linearGradient><linearGradient id="paint2_linear_10_23" x1="2.5" y1="2.5" x2="12" y2="10" gradientUnits="userSpaceOnUse"><stop stop-color="#85E29A"></stop><stop offset="1" stop-color="#34A853"></stop></linearGradient></defs></svg>"""

APP_CSS = """
<style>
@keyframes gemini-gradient { 0% {background-position: 0% 50%;} 50% {background-position: 100% 50%;} 100% {background-position: 0% 50%;} }
.gemini-title { font-size: 2.5em !important; font-weight: bold; background: linear-gradient(to right, #4285F4, #DB4437, #F4B400, #0F9D58, #4285F4); -webkit-background-clip: text; background-clip: text; color: transparent; background-size: 200% auto; animation: gemini-gradient 5s ease-in-out infinite; margin: 0; }
.log-panel .gradio-textbox { border: 2px solid #444; background-color: #1a1a1a; color: #00ff99; font-family: 'Courier New', Courier, monospace; }
</style>
"""

# --- å®¢æˆ·ç«¯åˆå§‹åŒ– ---
# æ–‡æ¡£å¼•ç”¨: "æäº¤ç¬¬ä¸€ä¸ªè¯·æ±‚"
# from google import genai
# client = genai.Client()
try:
    client = genai.Client()
except Exception as e:
    client = None

# --- 2. Pydantic Schema å®šä¹‰ (ç”¨äºåˆå§‹æ£€æµ‹) ---
# æ–‡æ¡£å¼•ç”¨: "ç”Ÿæˆ JSON"
# from pydantic import BaseModel
class ImageContext(BaseModel):
    is_relevant: bool
    scene_chinese: str
    reasoning: str

class BoundingBox(BaseModel):
    y_min: int
    x_min: int
    y_max: int
    x_max: int

class DetectedItem(BaseModel):
    label: str
    description: str
    confidence: float
    box_2d: BoundingBox
    suggested_tags: Optional[List[str]] = Field(default_factory=list)

# --- 3. æ ¸å¿ƒåç«¯é€»è¾‘ ---

# ã€V20 ä¿®å¤ã€‘
def call_gemini_api(model, prompt, image, pydantic_type):
    """
    (ç”¨äºåˆå§‹åˆ†æ) å•è½®APIè°ƒç”¨ï¼Œè¿”å›ç»“æ„åŒ– Pydantic å¯¹è±¡ã€‚
    ã€V20 ä¿®å¤ã€‘: è¿˜åŸä¸ºä½¿ç”¨ TypeAdapter æ‰‹åŠ¨ç”Ÿæˆ JSON Schemaï¼Œ
    ä»¥è§£å†³ `Unsupported schema type: ... type=None` é”™è¯¯ã€‚
    """
    if not client: return {"error": "Gemini Clientæœªåˆå§‹åŒ–ã€‚"}
    try:
        # 1. ã€å…³é”®ä¿®å¤ã€‘ä½¿ç”¨ TypeAdapter æ‰‹åŠ¨åˆ›å»º JSON Schema å­—å…¸ã€‚
        #    è¿™æ˜¯æ‚¨æœ€åˆä»£ç ä¸­çš„åšæ³•ï¼Œä¹Ÿæ˜¯æœ€å¥å£®çš„æ–¹æ³•ã€‚
        adapter = TypeAdapter(pydantic_type)
        schema_dict = adapter.json_schema()

        # 2. ä¸¥æ ¼éµå¾ª "ç»“æ„åŒ–è¾“å‡º" -> "ç”Ÿæˆ JSON" æ–‡æ¡£ï¼Œ
        #    ä¼ é€’ä¸€ä¸ª *åŒ…å«* "response_schema" é”®çš„ Python å­—å…¸ã€‚
        config_dict = {
            "response_mime_type": "application/json",
            "response_schema": schema_dict # <-- ä¼ é€’ç”Ÿæˆçš„å­—å…¸ï¼Œè€Œä¸æ˜¯ç±»å‹
        }

        # 3. æ–‡æ¡£å¼•ç”¨: "æäº¤ç¬¬ä¸€ä¸ªè¯·æ±‚" -> client.models.generate_content(...)
        response = client.models.generate_content(
            model=model, 
            contents=[image, prompt], # <-- åˆæ³•çš„ [Image, str] æ ¼å¼
            config=config_dict # <-- ä¼ é€’åŒ…å« schema å­—å…¸çš„ config
        )
        
        # 4. ã€å…³é”®ä¿®å¤ã€‘ä½¿ç”¨ TypeAdapter è§£æ JSON æ–‡æœ¬ã€‚
        #    æ–‡æ¡£æŒ‡å‡º .parsed å¯èƒ½ä¼šé™é»˜å¤±è´¥ã€‚
        #    æ‚¨æœ€åˆçš„ `adapter.validate_json` æ–¹æ³•æ›´å¥å£®ã€‚
        return adapter.validate_json(response.text)
    
    except Exception as e:
        # å°† Pydantic éªŒè¯é”™è¯¯æš´éœ²ç»™æ—¥å¿—
        print(f"[call_gemini_api FAILED] Model: {model}, Error: {str(e)}")
        return {"error": f"APIè°ƒç”¨å¤±è´¥: {str(e)}"}


def process_image(uploaded_image):
    """
    å¤„ç†å›¾åƒä¸Šä¼ ï¼Œè¿è¡Œåˆ†è¯Šï¼Œæ£€æµ‹ï¼Œå¹¶ã€åˆ›å»ºå’Œå¯åŠ¨ã€‘ä¸€ä¸ªæ–°çš„èŠå¤©ä¼šè¯ã€‚
    """
    if uploaded_image is None:
        # (è¾“å‡º: log, image, analysis_state, scene_state, chat_panel, chatbot_ui, chat_session_state)
        yield "è¯·å…ˆä¸Šä¼ ä¸€å¼ å›¾ç‰‡ã€‚", None, [], "", gr.update(visible=False), [], None
        return

    pil_image = Image.fromarray(uploaded_image).convert("RGB")
    log_text = "åˆå§‹åŒ–åˆ†ææµç¨‹...\n"
    yield log_text, (pil_image, []), [], "", gr.update(visible=False), [], None

    # --- é˜¶æ®µ 1: åˆ†è¯Š (ç»“æ„åŒ–è¾“å‡º) ---
    log_text += "----------\n[é˜¶æ®µ 1/2] è°ƒç”¨ Gemini 2.5 Flash è¿›è¡Œè¯¾é¢˜ç›¸å…³æ€§åˆ†è¯Š...\n"
    yield log_text, (pil_image, []), [], "", gr.update(visible=False), [], None
    
    triage_prompt = """
    ä½ æ˜¯ä¸€ä½ä¸¥æ ¼çš„AIåŠ©æ•™ï¼Œè´Ÿè´£è¯„å®¡ä¸€ç¯‡æœ¬ç§‘æ¯•ä¸šè®¾è®¡ã€‚
    è¯¾é¢˜åç§°ï¼šã€ŠåŸºäºç¤¾äº¤åª’ä½“æ•°æ®çš„ç”¨æˆ·å¥åº·ç”»åƒæ„å»ºã€‹ã€‚
    æ ¸å¿ƒç›®æ ‡ï¼šé€šè¿‡åˆ†æç”¨æˆ·åœ¨ç¤¾äº¤åª’ä½“ä¸Šå‘å¸ƒçš„å›¾ç‰‡ï¼Œæ¨æ–­å…¶å¥åº·å’Œç”Ÿæ´»æ–¹å¼ã€‚
    ä½ çš„ä»»åŠ¡ï¼šåˆ¤æ–­å½“å‰ä¸Šä¼ çš„è¿™å¼ å›¾ç‰‡æ˜¯å¦ç¬¦åˆè¯¥è¯¾é¢˜çš„ç ”ç©¶èŒƒå›´ã€‚
    
    ç¬¦åˆæ ‡å‡†çš„å›¾ç‰‡åº”å…·å¤‡ä»¥ä¸‹ç‰¹å¾ï¼š
    1.  **çœŸå®ç”Ÿæ´»åœºæ™¯**: å¿…é¡»æ˜¯çœŸå®æ‹æ‘„çš„ã€ä¸ä¸ªäººç”Ÿæ´»ç›´æ¥ç›¸å…³çš„ç…§ç‰‡ã€‚
    2.  **å¯æ¨æ–­æ€§**: å›¾ç‰‡å†…å®¹èƒ½æ˜ç¡®æˆ–é—´æ¥åœ°åæ˜ å‡ºä»¥ä¸‹è‡³å°‘ä¸€ç§ä¿¡æ¯ï¼š
        - **é¥®é£Ÿä¹ æƒ¯**: æ‹æ‘„äº†æ­£é¤ã€é›¶é£Ÿã€é¥®æ–™ç­‰ã€‚
        - **è¿åŠ¨çŠ¶æ€**: å¥èº«æˆ¿æ‰“å¡ã€æˆ·å¤–è·‘æ­¥ã€çƒç±»è¿åŠ¨ç­‰ã€‚
        - **ä½œæ¯æ¨¡å¼**: èƒ½çœ‹å‡ºæ˜¯æ¸…æ™¨æˆ–æ·±å¤œçš„æ´»åŠ¨ï¼Œä¾‹å¦‚æ·±å¤œåœ¨ç”µè„‘å‰å·¥ä½œï¼ˆå¯æ¨æ–­ä¸º'ç†¬å¤œ'ï¼‰ã€‚
        - **ç¤¾äº¤æ´»åŠ¨**: èšä¼šã€é¥®é…’ç­‰åœºæ™¯ã€‚
        - **æƒ…ç»ªæš—ç¤º**: æ•´ä½“ç¯å¢ƒæˆ–äººç‰©çŠ¶æ€èƒ½æš—ç¤ºæƒ…ç»ªã€‚

    ä¸ç¬¦åˆæ ‡å‡†çš„å›¾ç‰‡åŒ…æ‹¬ï¼š
    - çº¯é£æ™¯ç…§ã€ç½‘ç»œæ¢—å›¾ã€åŠ¨æ¼«æˆªå›¾ã€å±å¹•å½•å±ã€æŠ½è±¡è‰ºæœ¯å“ã€å¹¿å‘Šå®£ä¼ å›¾ç­‰ã€‚

    è¯·ä¸¥æ ¼æŒ‰ç…§æŒ‡å®šçš„JSONæ ¼å¼ï¼Œç”¨ä¸­æ–‡è¿”å›ä½ çš„è¯„å®¡ç»“æœã€‚
    """
    # ã€V16ã€‘è°ƒç”¨å·²ä¿®å¤çš„ call_gemini_api
    triage_result = call_gemini_api('gemini-2.5-flash', triage_prompt, pil_image, ImageContext)

    if isinstance(triage_result, dict) or not hasattr(triage_result, 'is_relevant') or not triage_result.is_relevant:
        reason = triage_result.reasoning if hasattr(triage_result, 'reasoning') else triage_result.get('error', 'æœªçŸ¥é”™è¯¯')
        log_text += f"âŒ åˆ†è¯Šã€ä¸é€šè¿‡ã€‘ã€‚\nè¯„å®¡ç†ç”±: {reason}\næµç¨‹ç»ˆæ­¢ã€‚"
        yield log_text, (pil_image, []), [], "", gr.update(visible=False), [], None
        return
    
    log_text += f"âœ… åˆ†è¯Šã€é€šè¿‡ã€‘ï¼\nåœºæ™¯åˆ¤æ–­: {triage_result.scene_chinese}\n"
    yield log_text, (pil_image, []), [], triage_result.scene_chinese, gr.update(visible=False), [], None
    time.sleep(1)

    # --- é˜¶æ®µ 2: æ·±åº¦åˆ†æ (ç»“æ„åŒ–è¾“å‡º) ---
    log_text += "----------\n[é˜¶æ®µ 2/2] è°ƒç”¨ Gemini 2.5 Pro è¿›è¡Œæ·±åº¦åˆ†æä¸æ ‡æ³¨...\n"
    yield log_text, (pil_image, []), [], triage_result.scene_chinese, gr.update(visible=False), [], None

    detection_prompt = f"""
    ä½ æ˜¯ä¸€ä½èµ„æ·±çš„å¥åº·æ•°æ®åˆ†æå¸ˆï¼Œæ­£åœ¨ä¸ºä¸€ä¸ªåä¸ºã€ŠåŸºäºç¤¾äº¤åª’ä½“æ•°æ®çš„ç”¨æˆ·å¥åº·ç”»åƒæ„å»ºã€‹çš„æ¯•ä¸šè®¾è®¡é¡¹ç›®å·¥ä½œã€‚
    å·²çŸ¥å½“å‰å›¾ç‰‡åœºæ™¯æ˜¯å…³äºâ€œ{triage_result.scene_chinese}â€ã€‚
    ä½ çš„æ ¸å¿ƒä»»åŠ¡æ˜¯ï¼šä»è¿™å¼ å›¾ç‰‡ä¸­**æ¦¨å–æ‰€æœ‰ä¸ä¸ªäººå¥åº·ã€æƒ…ç»ªã€å’Œè¡Œä¸ºæ¨¡å¼ç›¸å…³çš„ä¿¡å·**ï¼Œè€Œä¸ä»…ä»…æ˜¯è¯†åˆ«ç‰©ä½“ã€‚

    è¯·éµå¾ªä»¥ä¸‹ä¸¥æ ¼æŒ‡ä»¤ï¼š
    1.  **å…¨é¢æ£€æµ‹**: è¯¦ç»†æ£€æµ‹å›¾ä¸­çš„æ‰€æœ‰å…³é”®å…ƒç´ ã€‚
    2.  **æƒ…æ™¯åŒ–æ ‡ç­¾**: æ ‡ç­¾(label)ä¸ä»…è¦è¯´æ˜ç‰©ä½“æ˜¯ä»€ä¹ˆï¼Œæ›´è¦ä½“ç°å…¶åœ¨å¥åº·ç”»åƒä¸­çš„æ„ä¹‰ã€‚ä¾‹å¦‚ï¼Œä¸€ç¢—ç±³é¥­æ˜¯â€œé¥®é£Ÿ-ä¸»é£Ÿâ€ï¼Œä¸€ä¸ªå“‘é“ƒæ˜¯â€œè¿åŠ¨-å™¨æ¢°â€ï¼Œæ·±å¤œçš„ç”µè„‘æ˜¯â€œä½œæ¯-ç†¬å¤œè¿¹è±¡â€ã€‚
    3.  **æ·±åº¦æè¿°**: æè¿°(description)éœ€è¦ç®€è¦åˆ†æè¯¥å…ƒç´ å¯¹å¥åº·ç”»åƒçš„æ½œåœ¨è´¡çŒ®ã€‚
    4.  **æ™ºèƒ½æ¨æ–­æ ‡ç­¾ (Suggested Tags)**:
        - å¦‚æœç½®ä¿¡åº¦é«˜ï¼ˆ>0.9ï¼‰ï¼Œå¹¶ä¸”å¯ä»¥æ˜ç¡®æ¨æ–­å‡ºä¸€ç§å¥åº·ç›¸å…³çš„çŠ¶æ€ï¼ˆä¾‹å¦‚æ·±å¤œçš„ç”µè„‘å±å¹•å¯æ¨æ–­ä¸º'ç†¬å¤œ'ï¼Œä¸€æ¯é…’å¯æ¨æ–­ä¸º'é¥®é…’'ï¼‰ï¼Œè¯·åœ¨`suggested_tags`ä¸­æä¾›è¿™ä¸€ä¸ªç¡®å®šçš„çŠ¶æ€æ ‡ç­¾ã€‚
        - å¦‚æœç½®ä¿¡åº¦ä½ï¼ˆ<=0.9ï¼‰ï¼Œæˆ–è€…ç‰©ä½“æœ‰å¤šç§å¯èƒ½æ€§ï¼Œè¯·åœ¨`suggested_tags`ä¸­æä¾›2-3ä¸ªæœ€æœ‰å¯èƒ½çš„ä¸­æ–‡å€™é€‰æ ‡ç­¾ã€‚
    5.  **ä¸¥æ ¼æ ¼å¼**: ä½ çš„å›ç­”å¿…é¡»æ˜¯ä¸¥æ ¼éµå¾ªSchemaçš„JSONå¯¹è±¡åˆ—è¡¨ã€‚
    """
    # ã€V16ã€‘è°ƒç”¨å·²ä¿®å¤çš„ call_gemini_apiï¼Œä¼ é€’ List[DetectedItem] ç±»å‹
    detection_result = call_gemini_api('gemini-2.5-pro', detection_prompt, pil_image, List[DetectedItem])

    if isinstance(detection_result, dict) or not detection_result:
        error_msg = detection_result.get('error', 'æœªèƒ½æ£€æµ‹åˆ°ä»»ä½•ç‰©ä½“ã€‚')
        log_text += f"âŒ æ·±åº¦åˆ†æå¤±è´¥ã€‚\nç†ç”±: {error_msg}\næµç¨‹ç»ˆæ­¢ã€‚"
        yield log_text, (pil_image, []), [], triage_result.scene_chinese, gr.update(visible=False), [], None
        return
        
    log_text += f"âœ… æ·±åº¦åˆ†ææˆåŠŸï¼å…±è¯†åˆ«å‡º {len(detection_result)} ä¸ªå…³é”®å…ƒç´ ã€‚\n----------\nåˆ†æå…¨éƒ¨å®Œæˆã€‚"

    # --- é˜¶æ®µ 3: åˆ›å»ºå¹¶å¯åŠ¨èŠå¤©ä¼šè¯ (V15 æ¶æ„) ---
    
    # 1. (ä¸å˜) ç”Ÿæˆæ ‡æ³¨
    annotations = []
    w, h = pil_image.size
    for i, item in enumerate(detection_result):
        box = item.box_2d
        label_text = f"[{i}] {item.label}"
        if item.suggested_tags: label_text += f" ({', '.join(item.suggested_tags)})"
        annotation_box = (int(box.x_min/1000*w), int(box.y_min/1000*h), int(box.x_max/1000*w), int(box.y_max/1000*h))
        annotations.append((annotation_box, label_text))
    final_annotated_image = (pil_image, annotations)
    
    # 2. (ä¸å˜) å‡†å¤‡ *UI* çš„ç¬¬ä¸€æ¡æ¶ˆæ¯
    initial_summary = f"åˆ†æå®Œæˆï¼æˆ‘åˆ¤æ–­è¿™æ˜¯ä¸€ä¸ª**{triage_result.scene_chinese}**çš„åœºæ™¯ã€‚å›¾ç‰‡ä¸Šçš„æ ‡ç­¾æ˜¯æˆ‘æ£€æµ‹åˆ°çš„å…³é”®å…ƒç´  (å·²ç¼–å·)ï¼Œæ‚¨å¯ä»¥ç‚¹å‡»ä»»æ„ä¸€ä¸ªä¸æˆ‘äº’åŠ¨ã€‚"
    initial_chatbot_history = [{"role": "assistant", "content": initial_summary}]

    try:
        # 3. åˆ›å»ºèŠå¤©ä¼šè¯
        # æ–‡æ¡£å¼•ç”¨: "å¤šè½®å¯¹è¯ï¼ˆèŠå¤©ï¼‰" -> chat = client.chats.create(...)
        chat_session = client.chats.create(model="gemini-2.5-flash")
        
        # 4. å‡†å¤‡â€œè™šæ‹Ÿç¬¬0è½®â€çš„ç»Ÿä¸€æç¤ºï¼ˆå›¾åƒ + æ ‡ç­¾åˆ—è¡¨ï¼‰
        all_labels_summary = "\n".join([
            f"  - [{i}] {item.label}: {item.description}" 
            for i, item in enumerate(detection_result)
        ])
        unified_text_prompt = f"""
        è¿™æ˜¯æˆ‘ä¸Šä¼ çš„å›¾ç‰‡ã€‚è¯·ä½ ä»¥è¿™å¼ å›¾ç‰‡ä¸ºå”¯ä¸€çš„ã€å®Œæ•´çš„ä¸Šä¸‹æ–‡ã€‚ä¹‹åæˆ‘æ‰€æœ‰çš„å¯¹è¯ï¼ˆåŒ…æ‹¬ç‚¹å‡»æ ‡ç­¾ï¼‰éƒ½æ˜¯å…³äºè¿™å¼ å›¾çš„ã€‚è¯·åŸºäºå…¨å›¾è¿›è¡Œåˆ†æå’Œå›å¤ã€‚

        ã€ç³»ç»Ÿåˆå§‹ä¸Šä¸‹æ–‡ã€‘
        ä½ å·²ç»åˆ†æäº†è¿™å¼ å›¾ï¼Œè¿™æ˜¯ä½ çš„åˆ†æç»“æœï¼š
        - **æ•´ä½“åœºæ™¯åˆ¤æ–­:** {triage_result.scene_chinese}
        - **æ£€æµ‹åˆ°çš„å…³é”®å…ƒç´ åˆ—è¡¨ (ç´¢å¼•, æ ‡ç­¾, æè¿°):**
{all_labels_summary}

        è¯·ç‰¢è®°è¿™ä¸ªåˆ—è¡¨ã€‚ç”¨æˆ·æ¥ä¸‹æ¥çš„æ‰€æœ‰äº’åŠ¨ï¼ˆæ— è®ºæ˜¯ç‚¹å‡»è¿˜æ˜¯èŠå¤©ï¼‰éƒ½å°†åŸºäºè¿™å¼ å›¾å’Œè¿™ä¸ªåˆ—è¡¨ã€‚è¯·å›å¤â€œæ”¶åˆ°â€ä»¥ç¡®è®¤ã€‚
        """
        
        # 5. "å¯åŠ¨"ä¼šè¯ï¼šå‘é€å›¾åƒå’Œä¸Šä¸‹æ–‡ä½œä¸ºç¬¬ä¸€æ¡æ¶ˆæ¯
        # æ–‡æ¡£å¼•ç”¨: "å¤šè½®å¯¹è¯ï¼ˆèŠå¤©ï¼‰" -> chat.send_message(...)
        # æ–‡æ¡£å¼•ç”¨: "å¤šæ¨¡æ€è¾“å…¥" -> contents=[image, "Tell me about this instrument"]
        #    (chat.send_message æ˜¯ generate_content çš„å°è£…å™¨ï¼Œæ”¯æŒæ­¤ [Image, str] æ ¼å¼)
        priming_response = chat_session.send_message([pil_image, unified_text_prompt])
        log_text += f"\n[V16] èŠå¤©ä¼šè¯å·²å¯åŠ¨ã€‚AI å¯åŠ¨å›å¤: {priming_response.text}"
        
        # 6. æˆåŠŸï¼Œè¿”å›æ‰€æœ‰çŠ¶æ€
        yield (
            log_text, 
            final_annotated_image, 
            detection_result, 
            triage_result.scene_chinese, 
            gr.update(visible=True), 
            initial_chatbot_history,
            chat_session # <-- ã€V15ã€‘è¾“å‡ºæœ‰çŠ¶æ€çš„èŠå¤©å¯¹è±¡
        )
        
    except Exception as e:
        log_text += f"\nâŒ [V16] èŠå¤©ä¼šè¯å¯åŠ¨å¤±è´¥: {e}"
        yield (
            log_text, 
            final_annotated_image, 
            detection_result, 
            triage_result.scene_chinese, 
            gr.update(visible=True), 
            initial_chatbot_history, # UI ä»ç„¶æ˜¾ç¤º
            None # èŠå¤©ä¼šè¯å¤±è´¥
        )


def _regenerate_annotations(original_numpy_image, analysis_result_list: List[DetectedItem]):
    """(ä¸å˜) è¾…åŠ©å‡½æ•°ï¼šä»Numpyå›¾åƒå’Œç»“æœåˆ—è¡¨é‡æ–°ç”Ÿæˆå¸¦ç´¢å¼•çš„æ ‡æ³¨"""
    if original_numpy_image is None: return None
    original_image_pil = Image.fromarray(original_numpy_image).convert("RGB")
    w, h = original_image_pil.size
    new_annotations = []
    for i, item in enumerate(analysis_result_list):
        box = item.box_2d; label_text = f"[{i}] {item.label}"
        if item.suggested_tags: label_text += f" ({', '.join(item.suggested_tags)})"
        annotation_box = (int(box.x_min/1000*w), int(box.y_min/1000*h), int(box.x_max/1000*w), int(box.y_max/1000*h))
        new_annotations.append((annotation_box, label_text))
    return (original_image_pil, new_annotations)


def handle_select(state_analysis_result: List[DetectedItem], 
                  chat_history: list, 
                  state_chat_session, # <-- ã€V18 ä¿®å¤ã€‘ç§»é™¤ç±»å‹æç¤º
                  evt: gr.SelectData):
    """
    ã€å·²é‡æ„ v16 - ä½¿ç”¨ Chat APIã€‘
    å¤„ç†ç”¨æˆ·ç‚¹å‡»æ ‡ç­¾äº‹ä»¶çš„å‡½æ•°ã€‚
    """

    if state_chat_session is None:
        chat_history.append({"role": "assistant", "content": "æŠ±æ­‰ï¼ŒèŠå¤©ä¼šè¯æœªåˆå§‹åŒ–ï¼Œè¯·é‡æ–°ä¸Šä¼ å›¾ç‰‡ã€‚"})
        return chat_history, None, state_chat_session # è¿”å› None chat_session

    output_selected_object = None # é»˜è®¤ä¸æ”¹å˜
    
    # (V4 ä¿®å¤) å¿½ç•¥éæ ‡æ³¨æ¡†ç‚¹å‡»
    if evt.index is None:
        print(f"Ignored select event: No index provided (e.g., title click). Value was: {evt.value}")
        return chat_history, None, state_chat_session
    
    try:
        selected_index = evt.index
        
        if 0 <= selected_index < len(state_analysis_result):
            selected_object = state_analysis_result[selected_index]
            
            # 1. å‡†å¤‡ç”¨æˆ·æ¶ˆæ¯ å’Œ ç³»ç»ŸæŒ‡ä»¤
            user_click_message = f"ï¼ˆæˆ‘ç‚¹å‡»äº†æ ‡ç­¾ [{selected_index}]: â€œ{selected_object.label}â€ï¼‰"
            system_prompt_for_click = f"""
            ä½ æ˜¯ä¸€ä¸ªå¯Œæœ‰åŒç†å¿ƒä¸”æ´å¯ŸåŠ›æ•é”çš„æ™ºèƒ½åŠ©æ‰‹ã€‚
            ã€ä¸Šä¸‹æ–‡ã€‘ä½ å·²ç»çœ‹åˆ°äº†å®Œæ•´çš„å›¾ç‰‡å’Œæ‰€æœ‰æ£€æµ‹å…ƒç´ çš„åˆ—è¡¨ã€‚
            ã€ç”¨æˆ·æ“ä½œã€‘ç”¨æˆ·åˆšåˆšåœ¨å›¾ç‰‡ä¸Šç‚¹å‡»äº† ç¼–å·ä¸º [{selected_index}] çš„ç‰©ä½“â€œ{selected_object.label}â€ï¼ˆåŸºç¡€æè¿°ï¼šâ€œ{selected_object.description}â€ï¼‰ã€‚
            ã€ä½ çš„ä»»åŠ¡ã€‘è¯·ç»“åˆ**æ•´å¼ å›¾ç‰‡**çš„ä¸Šä¸‹æ–‡ï¼ˆä¸ä»…ä»…æ˜¯æ ‡ç­¾ï¼Œåˆ©ç”¨ä½ çœ‹åˆ°çš„å…¨å›¾ï¼‰å’Œ**å®Œæ•´çš„æ£€æµ‹åˆ—è¡¨**ï¼Œä¸»åŠ¨æ¨æµ‹ç”¨æˆ·å¯èƒ½å…³å¿ƒçš„é—®é¢˜ï¼Œæˆ–è€…ç»™å‡ºä¸€ä¸ªæœ‰è¶£çš„ã€ç›¸å…³çš„æ´å¯Ÿã€‚ä½ çš„å›å¤è¦è‡ªç„¶ã€å‹å¥½ï¼Œåƒæ˜¯åœ¨å¼€å¯ä¸€æ®µå¯¹è¯ã€‚
            """
            
            # 2. å°†ç”¨æˆ·æ¶ˆæ¯å’Œç³»ç»ŸæŒ‡ä»¤åˆå¹¶ä¸º *ä¸€ä¸ª* å­—ç¬¦ä¸²
            final_user_prompt = f"{user_click_message}\n\n{system_prompt_for_click}"

            # 3. æ›´æ–° UI (æ˜¾ç¤ºç”¨æˆ·ç‚¹å‡» + åŠ è½½)
            chat_history.append({"role": "user", "content": user_click_message})
            chat_history.append({"role": "assistant", "content": "ğŸ¤” æ­£ åœ¨ ä¸º æ‚¨ åˆ† æ..."})

            # 4. è°ƒç”¨ Chat API
            # æ–‡æ¡£å¼•ç”¨: "å¤šè½®å¯¹è¯ï¼ˆèŠå¤©ï¼‰" -> response = chat.send_message("How many paws...")
            response = state_chat_session.send_message(final_user_prompt) # <-- å‘é€ [str]
            
            # 5. æ›´æ–° UI
            chat_history[-1] = {"role": "assistant", "content": response.text}
            output_selected_object = {"index": selected_index, "data": selected_object.model_dump()}
        else:
            print(f"Error: evt.index {selected_index} is out of bounds.")
            chat_history.append({"role": "assistant", "content": f"æŠ±æ­‰ï¼Œæˆ‘å†…éƒ¨å¥½åƒå‡ºé”™äº†ï¼Œæ‰¾ä¸åˆ°ç´¢å¼• {selected_index}ã€‚"})
            
    except Exception as e:
        print(f"Error in handle_select logic: {e}")
        error_message = f"æŠ±æ­‰ï¼Œå¤„ç†ç‚¹å‡»æ—¶å‡ºé”™: {str(e)}"
        if chat_history and chat_history[-1]["role"] == "assistant":
             chat_history[-1] = {"role": "assistant", "content": error_message}
        else:
            chat_history.append({"role": "assistant", "content": error_message})

    # è¿”å› UI æ›´æ–°, å¹¶å°† *åŒä¸€ä¸ª* chat_session å¯¹è±¡ä¼ å› state
    return chat_history, output_selected_object, state_chat_session


def handle_reply(state_analysis_result: List[DetectedItem], 
                 state_selected_object: dict, 
                 user_input: str, 
                 chat_history: list, 
                 original_numpy_image: np.ndarray,
                 state_chat_session): # <-- ã€V18 ä¿®å¤ã€‘ç§»é™¤ç±»å‹æç¤º
    """
    ã€å·²é‡æ„ v16 - ä½¿ç”¨ Chat APIã€‘
    å¤„ç†ç”¨æˆ·çš„æ–‡æœ¬å›å¤ã€‚
    """
    
    
    # (V2 ä¿®å¤) å¤„ç†ç©ºè¾“å…¥
    if not user_input:
        current_annotated_image = _regenerate_annotations(original_numpy_image, state_analysis_result)
        # (chat_history, chat_input, annotated_output, analysis_result, chat_session)
        return chat_history, "", current_annotated_image, state_analysis_result, state_chat_session

    if state_chat_session is None:
        chat_history.append({"role": "assistant", "content": "æŠ±æ­‰ï¼ŒèŠå¤©ä¼šè¯æœªåˆå§‹åŒ–ï¼Œè¯·é‡æ–°ä¸Šä¼ å›¾ç‰‡ã€‚"})
        current_annotated_image = _regenerate_annotations(original_numpy_image, state_analysis_result)
        return chat_history, "", current_annotated_image, state_analysis_result, state_chat_session

    # 1. å‡†å¤‡ç³»ç»ŸæŒ‡ä»¤
    selected_object_context = f"ç”¨æˆ·å½“å‰æ­£åœ¨è®¨è®ºçš„ç‰©ä½“æ˜¯ ç¼–å·[{state_selected_object['index']}] '{state_selected_object['data']['label']}'ã€‚" if state_selected_object else ""
    system_prompt_for_reply = f"""
    ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ã€‚
    ã€é‡è¦ä¸Šä¸‹æ–‡ã€‘ä½ **èƒ½çœ‹åˆ°æ•´å¼ å›¾ç‰‡**ï¼Œå¹¶ä¸”ä½ **å·²ç»æœ‰äº†ä¸€ä¸ªå®Œæ•´çš„æ£€æµ‹å…ƒç´ åˆ—è¡¨**ï¼ˆåœ¨å†å²è®°å½•çš„å¼€å¤´ï¼‰ã€‚è¯·å§‹ç»ˆç»“åˆ**å…¨å›¾ä¸Šä¸‹æ–‡**ã€**å®Œæ•´çš„æ£€æµ‹åˆ—è¡¨**ã€{selected_object_context} å’Œå¯¹è¯å†å²æ¥å›ç­”ã€‚
    
    ã€ç”¨æˆ·æœ€æ–°å›å¤ã€‘: "{user_input}"

    ä½ çš„ä»»åŠ¡æ˜¯:
    1.  **ç»§ç»­å¯¹è¯**: ç”Ÿæˆä¸€æ®µè‡ªç„¶çš„ã€æœ‰å¸®åŠ©çš„å›å¤ã€‚
    2.  **è¯†åˆ«ä¿®æ­£æ„å›¾**: å¦‚æœç”¨æˆ·çš„å›å¤æ˜ç¡®æŒ‡å‡ºäº†ä¸€ä¸ªè¯†åˆ«é”™è¯¯ï¼ˆä¾‹å¦‚â€œé‚£ä¸æ˜¯XXï¼Œæ˜¯YYâ€ï¼‰ï¼Œä½ å¿…é¡»åœ¨ä½ çš„å¯¹è¯å›å¤ä¹‹åï¼Œå¦èµ·ä¸€è¡Œï¼ŒåµŒå…¥ä¸€ä¸ªç‰¹å®šæ ¼å¼çš„JSONæŒ‡ä»¤å—æ¥ä¿®æ­£æ ‡ç­¾ã€‚
    3.  **è¿”å›ç´¢å¼•**: ä½ çš„JSONå¿…é¡»åŒ…å«è¢«ä¿®æ­£ç‰©ä½“çš„æ•°å­—ç´¢å¼• `index`ã€‚

    JSONæŒ‡ä»¤æ ¼å¼: `ACTION_JSON:{{"action": "update_label", "index": <è¢«ä¿®æ­£ç‰©ä½“çš„æ•°å­—ç¼–å·>, "new_label": "ç”¨æˆ·æä¾›çš„æ–°æ ‡ç­¾"}}`
    """
    
    # 2. å°†ç”¨æˆ· *å®é™…è¾“å…¥* å’Œç³»ç»ŸæŒ‡ä»¤åˆå¹¶ä¸º *ä¸€ä¸ª* å­—ç¬¦ä¸²
    final_user_prompt = f"{user_input}\n\n{system_prompt_for_reply}"
    
    # 3. æ›´æ–° UI (æ˜¾ç¤ºç”¨æˆ·è¾“å…¥ + åŠ è½½)
    chat_history.append({"role": "user", "content": user_input})
    chat_history.append({"role": "assistant", "content": "ğŸ¤” æ­£ åœ¨ ä¸º æ‚¨ åˆ† æ..."})
    
    try:
        # 4. è°ƒç”¨ Chat API
        # æ–‡æ¡£å¼•ç”¨: "å¤šè½®å¯¹è¯ï¼ˆèŠå¤©ï¼‰" -> response = chat.send_message("How many paws...")
        response = state_chat_session.send_message(final_user_prompt) # <-- å‘é€ [str]
        full_response_text = response.text
        
        # 5. æ›´æ–° UI (ä»…å¯¹è¯éƒ¨åˆ†)
        chat_history[-1] = {"role": "assistant", "content": full_response_text.split("ACTION_JSON:")[0].strip()}
        
        # (ä¸å˜) å¤„ç† ACTION_JSON çš„é€»è¾‘
        new_analysis_result = state_analysis_result
        if "ACTION_JSON:" in full_response_text:
            try:
                action_str = full_response_text.split("ACTION_JSON:")[1]
                action_data = json.loads(action_str)
                
                if action_data.get("action") == "update_label":
                    target_index = int(action_data['index'])
                    new_label = action_data['new_label']
                    
                    if 0 <= target_index < len(state_analysis_result):
                        truly_new_list = list(state_analysis_result) 
                        updated_item = truly_new_list[target_index].model_copy(
                            update={'label': new_label, 'suggested_tags': []} 
                        )
                        truly_new_list[target_index] = updated_item
                        new_analysis_result = truly_new_list
                    else:
                        print(f"AIè¿”å›çš„ç´¢å¼• {target_index} è¶…å‡ºèŒƒå›´ã€‚")
                        
            except Exception as e:
                print(f"è§£ææˆ–æ‰§è¡ŒActionæŒ‡ä»¤å¤±è´¥: {e}")

        # (ä¸å˜) é‡æ–°ç”Ÿæˆæ ‡æ³¨
        new_annotated_image = _regenerate_annotations(original_numpy_image, new_analysis_result)
                
        return chat_history, "", new_annotated_image, new_analysis_result, state_chat_session
        
    except Exception as e:
        # æ•è· API é”™è¯¯
        print(f"Error in handle_reply logic: {e}")
        error_message = f"æŠ±æ­‰ï¼Œå¤„ç†å›å¤æ—¶å‡ºé”™: {str(e)}"
        chat_history[-1] = {"role": "assistant", "content": error_message}
        # Gradio é”™è¯¯ä¿®å¤ï¼šä¿æŒå›¾åƒå’ŒçŠ¶æ€ä¸å˜
        current_annotated_image = _regenerate_annotations(original_numpy_image, state_analysis_result)
        return chat_history, "", current_annotated_image, state_analysis_result, state_chat_session


# --- 5. æ„å»º Gradio Web ç•Œé¢ ---
with gr.Blocks(theme='gradio/dracula_revamped') as demo:
    gr.HTML(APP_CSS)
    gr.HTML(f'<div style="display: flex; align-items: center;">{GEMINI_LOGO_SVG}<h1 class="gemini-title">åŠ¨æ€å¥åº·å­ªç”Ÿæ™ºèƒ½ä½“ v16.0 </h1></div>')
    
    # çŠ¶æ€ï¼šå­˜å‚¨Pydanticå¯¹è±¡åˆ—è¡¨ List[DetectedItem]
    state_analysis_result = gr.State([])
    # çŠ¶æ€ï¼šå­˜å‚¨åœºæ™¯å­—ç¬¦ä¸²
    state_scene = gr.State("")
    # çŠ¶æ€ï¼šå­˜å‚¨è¢«é€‰ä¸­å¯¹è±¡çš„ä¿¡æ¯ {"index": int, "data": dict}
    state_selected_object = gr.State(None)
    # ã€V15ã€‘å­˜å‚¨æœ‰çŠ¶æ€çš„ chat SDK å¯¹è±¡
    state_chat_session = gr.State(None) 

    with gr.Row():
        with gr.Column(scale=3):
            # image_input (Numpy) æ˜¯æˆ‘ä»¬ç»Ÿä¸€ä¸Šä¸‹æ–‡çš„å›¾åƒæº
            image_input = gr.Image(type="numpy", label="ä¸Šä¼ æ‚¨çš„ç”Ÿæ´»ç…§ç‰‡")
            submit_btn = gr.Button("å¼€å§‹åˆ†æ", variant="primary")
            annotated_output = gr.AnnotatedImage(label="åˆ†æç»“æœï¼ˆç‚¹å‡»å¸¦ç¼–å·çš„æ ‡ç­¾äº’åŠ¨ï¼‰", height=400)

        with gr.Column(scale=2):
            log_box = gr.Textbox(label="âš™ï¸ å®Œæ•´æŠ€æœ¯æ—¥å¿—", lines=15, interactive=False, elem_classes=["log-panel"])
            with gr.Group(visible=False) as interaction_panel:
                chatbot = gr.Chatbot(label="æ™ºèƒ½äº¤äº’åŒº (å·²è¿æ¥ Chat API)", height=350, type="messages", avatar_images=("user.png", "gemini.png"))
                with gr.Row():
                    chat_input = gr.Textbox(show_label=False, scale=9, interactive=True, placeholder="è¾“å…¥æ‚¨çš„å›å¤æˆ–ä¿®æ­£...")
                    chat_submit_btn = gr.Button("å‘é€", variant="primary", scale=1)

    # äº‹ä»¶ç»‘å®š
    submit_btn.click(
        fn=process_image,
        inputs=[image_input],
        outputs=[
            log_box, 
            annotated_output, 
            state_analysis_result, 
            state_scene, 
            interaction_panel, 
            chatbot,
            state_chat_session # <-- ã€V15 ç»‘å®šã€‘
        ]
    )
    
    annotated_output.select(
        fn=handle_select,
        inputs=[
            state_analysis_result, 
            chatbot, 
            state_chat_session, # <-- ã€V15 ç»‘å®šã€‘
        ],
        outputs=[
            chatbot, 
            state_selected_object,
            state_chat_session # <-- ã€V15 ç»‘å®šã€‘
        ]
    )

    chat_submit_btn.click(
        fn=handle_reply,
        inputs=[
            state_analysis_result, 
            state_selected_object, 
            chat_input, 
            chatbot, 
            image_input, # <-- _regenerate_annotations ä»éœ€è¦å®ƒ
            state_chat_session # <-- ã€V15 ç»‘å®šã€‘
        ],
        outputs=[
            chatbot, 
            chat_input, 
            annotated_output, 
            state_analysis_result,
            state_chat_session # <-- ã€V15 ç»‘å®šã€‘
        ]
    )

if __name__ == "__main__":
    if client:
        # ç¡®ä¿å¤´åƒæ–‡ä»¶å­˜åœ¨
        if not os.path.exists("user.png"): Image.new('RGB', (100, 100), color = 'dodgerblue').save('user.png')
        if not os.path.exists("gemini.png"): Image.new('RGB', (100, 100), color = '#7e57c2').save('gemini.png')
        demo.launch(share=True, debug=True)
    else:
        print("\nåº”ç”¨æ— æ³•å¯åŠ¨ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒå˜é‡ã€‚")